{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copia de C3W2_Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Week 2 Assignment: Zombie Detection\n",
        "\n",
        "Welcome to this week's programming assignment! You will use the Object Detection API and retrain [RetinaNet](https://arxiv.org/abs/1708.02002) to spot Zombies using just 5 training images. You will setup the model to restore pretrained weights and fine tune the classification layers.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=18Ck0qNSZy9F1KsUKWc4Jv7_x_1e_fXTN' alt='zombie'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# clone the Tensorflow Model Garden\n",
        "!git clone --depth 1 https://github.com/tensorflow/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# install the Object Detection API\n",
        "!cd models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21tUtyyVrkIt"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZcqD4NLdnf4"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "from object_detection.utils import colab_utils\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "You'll define a couple of utility functions for loading images and plotting detections. The code is provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "    path: a file path.\n",
        "\n",
        "    Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    \n",
        "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(img_data))\n",
        "    (im_width, im_height) = image.size\n",
        "    \n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "    \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "    Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "          and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "          this function assumes that the boxes to be plotted are groundtruth\n",
        "          boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "          category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_np_with_annotations = image_np.copy()\n",
        "    \n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_annotations,\n",
        "        boxes,\n",
        "        classes,\n",
        "        scores,\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        min_score_thresh=0.8)\n",
        "    \n",
        "    if image_name:\n",
        "        plt.imsave(image_name, image_np_with_annotations)\n",
        "    \n",
        "    else:\n",
        "        plt.imshow(image_np_with_annotations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSaXL28TZfk1"
      },
      "source": [
        "## Download the Zombie data\n",
        "\n",
        "Now you will get 5 images of zombies that you will use for training. \n",
        "- The zombies are hosted in a Google bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSFoJz2Cgzs"
      },
      "source": [
        "# download the images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training-zombie.zip \\\n",
        "    -O ./training-zombie.zip\n",
        "\n",
        "# unzip to a local directory\n",
        "local_zip = './training-zombie.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./training')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzSGUDsrkI7"
      },
      "source": [
        "<a name='exercise-2'></a>\n",
        "\n",
        "### **Exercise 2**: Visualize the training images\n",
        "\n",
        "Next, you'll want to inspect the images that you just downloaded. \n",
        "\n",
        "* Please replace instances of `None` below to load and visualize the 5 training images. \n",
        "* You can inspect the *training* directory (using the `Files` button on the left side of this Colab) to see the filenames of the zombie images. The paths for the images will look like this:\n",
        "\n",
        "```\n",
        "./training/training-zombie1.jpg\n",
        "./training/training-zombie2.jpg\n",
        "./training/training-zombie3.jpg\n",
        "./training/training-zombie4.jpg\n",
        "./training/training-zombie5.jpg\n",
        "```\n",
        "- To set file paths, you'll use [os.path.join](https://www.geeksforgeeks.org/python-os-path-join-method/).  As an example, if you wanted to create the path './parent_folder/file_name1.txt', you could write: \n",
        "\n",
        "`os.path.join('parent_folder', 'file_name', str(1), '.txt')`\n",
        "\n",
        "* You should see the 5 training images after running this cell. If not, please inspect your code, particularly the `image_path`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQy3ND7EpFQM"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "### START CODE HERE (Replace Instances of `None` with your code) ###\n",
        "\n",
        "# assign the name (string) of the directory containing the training images\n",
        "train_image_dir = './training'\n",
        "\n",
        "# declare an empty list\n",
        "train_images_np = []\n",
        "\n",
        "# run a for loop for each image\n",
        "for i in range(1, 6):\n",
        "\n",
        "    # define the path (string) for each image\n",
        "    image_path = os.path.join(train_image_dir,'training-zombie'+str(i)+'.jpg')\n",
        "    print(image_path)\n",
        "\n",
        "    # load images into numpy arrays and append to a list\n",
        "    train_images_np.append(load_image_into_numpy_array(image_path))\n",
        "### END CODE HERE ###\n",
        "\n",
        "# configure plot settings via rcParams\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['xtick.labelsize'] = False\n",
        "plt.rcParams['ytick.labelsize'] = False\n",
        "plt.rcParams['xtick.top'] = False\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.right'] = False\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# plot images\n",
        "for idx, train_image_np in enumerate(train_images_np):\n",
        "    plt.subplot(1, 5, idx+1)\n",
        "    plt.imshow(train_image_np)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqb_yjAo3cO_"
      },
      "source": [
        "<a name='gt_boxes_definition'></a>\n",
        "## Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqDVC6epheJZ"
      },
      "source": [
        "# Define the list of ground truth boxes\n",
        "gt_boxes = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNpBspiRheJZ"
      },
      "source": [
        "#### Option 1: draw your own ground truth boxes\n",
        "If you want to draw your own, please run the next cell and the following test code. If not, then skip these optional cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEDRoUEcUgL"
      },
      "source": [
        "# Option 1: draw your own ground truth boxes\n",
        "colab_utils.annotate(train_images_np, box_storage_pointer=gt_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsSVYhTVZZSP"
      },
      "source": [
        "# Option 1: draw your own ground truth boxes\n",
        "# TEST CODE:\n",
        "try:\n",
        "  assert(len(gt_boxes) == 5), \"Warning: gt_boxes is empty. Did you click `submit`?\"\n",
        "\n",
        "except AssertionError as e:\n",
        "  print(e)\n",
        "\n",
        "# checks if there are boxes for all 5 images\n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None), \"There are less than 5 sets of box coordinates. \" \\\n",
        "                                  \"Please re-run the cell above to draw the boxes again.\\n\" \\\n",
        "                                  \"Alternatively, you can run the next cell to load pre-determined \" \\\n",
        "                                  \"ground truth boxes.\"\n",
        "    \n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        break\n",
        "\n",
        "\n",
        "ref_gt_boxes = [\n",
        "        np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "        np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "        np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "        np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "        np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "      ]\n",
        "\n",
        "for gt_box, ref_gt_box in zip(gt_boxes, ref_gt_boxes):\n",
        "    try:\n",
        "      assert(np.allclose(gt_box, ref_gt_box, atol=0.04)), \"One of the boxes is too big or too small. \" \\\n",
        "                                                          \"Please re-draw and make the box tighter around the zombie.\"\n",
        "    \n",
        "    except AssertionError as e:\n",
        "      print(e)\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMC13HrOrkI_"
      },
      "source": [
        "<a name='gt-boxes'></a>\n",
        "#### Option 2: use the given ground truth boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDJadmvdrkI_"
      },
      "source": [
        "# Option 2: use given ground truth boxes\n",
        "# set this to `True` if you want to override the boxes you drew\n",
        "override = False\n",
        "\n",
        "# bounding boxes for each of the 5 zombies found in each image. \n",
        "# you can use these instead of drawing the boxes yourself.\n",
        "ref_gt_boxes = [\n",
        "        np.array([[0.27333333, 0.41500586, 0.74333333, 0.57678781]]),\n",
        "        np.array([[0.29833333, 0.45955451, 0.75666667, 0.61078546]]),\n",
        "        np.array([[0.40833333, 0.18288394, 0.945, 0.34818288]]),\n",
        "        np.array([[0.16166667, 0.61899179, 0.8, 0.91910903]]),\n",
        "        np.array([[0.28833333, 0.12543962, 0.835, 0.35052755]]),\n",
        "      ]\n",
        "\n",
        "# if gt_boxes is empty, use the reference\n",
        "if not gt_boxes or override is True:\n",
        "  gt_boxes = ref_gt_boxes\n",
        "\n",
        "# if gt_boxes does not contain 5 box coordinates, use the reference \n",
        "for gt_box in gt_boxes:\n",
        "    try:\n",
        "      assert(gt_box is not None)\n",
        "    \n",
        "    except:\n",
        "      gt_boxes = ref_gt_boxes\n",
        "      \n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olhzhCH5heJa"
      },
      "source": [
        "#### View your ground truth box coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxd66yq-M9Va"
      },
      "source": [
        "# print the coordinates of your ground truth boxes\n",
        "for gt_box in gt_boxes:\n",
        "  print(gt_box)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeDQaSdrkJC"
      },
      "source": [
        "<a name='exercise-3'></a>\n",
        "\n",
        "### Define the category index dictionary\n",
        "\n",
        "You'll need to tell the model which integer class ID to assign to the 'zombie' category, and what 'name' to associate with that integer id.\n",
        "\n",
        "- zombie_class_id: \n",
        "Since you are just predicting one class (zombie), please assign `1` to the zombie class ID.\n",
        "\n",
        "- category_index: Please define the `category_index` dictionary, which will have the same structure as this:\n",
        "```\n",
        "{human_class_id : \n",
        "  {'id'  : human_class_id, \n",
        "   'name': 'human_so_far'}\n",
        "}\n",
        "```\n",
        "  - Define `category_index` similar to the example dictionary above, except for zombies.\n",
        "  - This will be used by the succeeding functions to know the class `id` and `name` of zombie images.\n",
        "\n",
        "- num_classes: Since you are predicting one class, please assign `1` to the number of classes that the model will predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBqFVMcweF-"
      },
      "source": [
        "# Assign the zombie class ID\n",
        "zombie_class_id = 1\n",
        "\n",
        "# define a dictionary describing the zombie class\n",
        "category_index = {zombie_class_id : {'id': zombie_class_id, 'name':'zombie'}}\n",
        "\n",
        "# Specify the number of classes that the model will predict\n",
        "num_classes = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPy7vfCCrkJF"
      },
      "source": [
        "# TEST CODE:\n",
        "\n",
        "print(category_index[zombie_class_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL0nj1wvrkJH"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```txt\n",
        "{'id': 1, 'name': 'zombie'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvj7Q0oYk75"
      },
      "source": [
        "### Data preprocessing\n",
        "You will now do some data preprocessing so it is formatted properly before it is fed to the model:\n",
        "- Convert the class labels to one-hot representations\n",
        "- convert everything (i.e. train images, gt boxes and class labels) to tensors.\n",
        "\n",
        "This code is provided for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68ot6BkrkJH"
      },
      "source": [
        "# The `label_id_offset` here shifts all classes by a certain number of indices;\n",
        "# we do this here so that the model receives one-hot labels where non-background\n",
        "# classes start counting at the zeroth index.  This is ordinarily just handled\n",
        "# automatically in our training binaries, but we need to reproduce it here.\n",
        "\n",
        "label_id_offset = 1\n",
        "train_image_tensors = []\n",
        "\n",
        "# lists containing the one-hot encoded classes and ground truth boxes\n",
        "gt_classes_one_hot_tensors = []\n",
        "gt_box_tensors = []\n",
        "\n",
        "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
        "    \n",
        "    # convert training image to tensor, add batch dimension, and add to list\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
        "        train_image_np, dtype=tf.float32), axis=0))\n",
        "    \n",
        "    # convert numpy array to tensor, then add to list\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "    \n",
        "    # apply offset to to have zero-indexed ground truth classes\n",
        "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
        "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
        "    \n",
        "    # do one-hot encoding to ground truth classes\n",
        "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
        "        zero_indexed_groundtruth_classes, num_classes))\n",
        "\n",
        "print('Done prepping data.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3_Z3mJWN9KJ"
      },
      "source": [
        "## Visualize the zombies with their ground truth bounding boxes\n",
        "\n",
        "You should see the 5 training images with the bounding boxes after running the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBD6l-E4N71y"
      },
      "source": [
        "# give boxes a score of 100%\n",
        "dummy_scores = np.array([1.0], dtype=np.float32)\n",
        "\n",
        "# define the figure size\n",
        "plt.figure(figsize=(30, 15))\n",
        "\n",
        "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
        "for idx in range(5):\n",
        "    plt.subplot(2, 4, idx+1)\n",
        "    plot_detections(\n",
        "      train_images_np[idx],\n",
        "      gt_boxes[idx],\n",
        "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
        "      dummy_scores, category_index)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghDAsqfoZvPh"
      },
      "source": [
        "## Download the checkpoint containing the pre-trained weights\n",
        "\n",
        "Next, you will download [RetinaNet](https://arxiv.org/abs/1708.02002) and copy it inside the object detection directory.\n",
        "\n",
        "  - Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.\n",
        "  - Untar (decompress) the tar file\n",
        "  - Move the decompressed checkpoint to `models/research/object_detection/test_data/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn21-EelheJa"
      },
      "source": [
        "<a name='exercise-4'></a>\n",
        "### Download checkpoints\n",
        "\n",
        "  - Download the compressed SSD Resnet 50 version 1, 640 x 640 checkpoint.\n",
        "  - Untar (decompress) the tar file\n",
        "  - Move the decompressed checkpoint to `models/research/object_detection/test_data/`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J16r3NChD-7"
      },
      "source": [
        "\n",
        "# Download the SSD Resnet 50 version 1, 640x640 checkpoint\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "   \n",
        "# untar (decompress) the tar file\n",
        "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "\n",
        "# copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgG_YT7UrkJQ"
      },
      "source": [
        "## Configure the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3eHyNmxheJa"
      },
      "source": [
        "<a name='exercise-5-1'></a>\n",
        "\n",
        "### Locate and read from the configuration file\n",
        "\n",
        "#### pipeline_config\n",
        "- Set the `pipeline_config` to a string that contains the full path to the resnet config file, in other words: `models/research/.../... .config`\n",
        "\n",
        "#### configs\n",
        "If you look at the module [config_util](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/config_util.py) that you imported, it contains the following function:\n",
        "\n",
        "```\n",
        "def get_configs_from_pipeline_file(pipeline_config_path, config_override=None):\n",
        "```\n",
        "- Please use this function to load the configuration from your `pipeline_config`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59sEM6wXheJa"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
        "\n",
        "# Load the configuration file into a dictionary\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "\n",
        "# See what configs looks like\n",
        "configs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAkgGnMcheJa"
      },
      "source": [
        "<a name='exercise-5-2'></a>\n",
        "\n",
        "###Get the model configuration\n",
        "\n",
        "#### model_config\n",
        "- From the `configs` dictionary, access the object associated with the key 'model'.\n",
        "It should look like this:\n",
        "  \n",
        "```\n",
        "ssd {\n",
        "  num_classes: 90\n",
        "  image_resizer {\n",
        "    fixed_shape_resizer {\n",
        "      height: 640\n",
        "      width: 640\n",
        "    }\n",
        "  }\n",
        "  feature_extractor {\n",
        "...\n",
        "...\n",
        "  freeze_batchnorm: false\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps3AzqBvheJa"
      },
      "source": [
        "# Read in the object stored at the key 'model' of the configs dictionary\n",
        "model_config = configs['model']\n",
        "\n",
        "# see what model_config looks like\n",
        "model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxHqQK1eheJa"
      },
      "source": [
        "<a name='exercise-5-3'></a>\n",
        "\n",
        "### Modify model_config\n",
        "- Modify num_classes from the default `90` to the `num_classes` that you set earlier in this notebook.\n",
        "- Freeze batch normalization \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyT4BUbaMeG-"
      },
      "source": [
        "# Modify the number of classes from its default of 90\n",
        "model_config.ssd.num_classes = num_classes\n",
        "\n",
        "# Freeze batch normalization\n",
        "model_config.ssd.freeze_batchnorm = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFnuT2rfheJa"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvtrqsJIrkJT"
      },
      "source": [
        "<a name='exercise-5.4'></a>\n",
        "\n",
        "### Build the custom model\n",
        "\n",
        "- model_config: Set this to the model configuration that you just customized.\n",
        "- is_training: Set this to True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfoRTeV_rkJT"
      },
      "source": [
        "detection_model = model_builder.build(model_config,is_training=True)\n",
        "\n",
        "print(type(detection_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuUBhoegrkJY"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```txt\n",
        "<class 'object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch'>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C97Zo0OJnOf"
      },
      "source": [
        "## Restore weights from your checkpoint\n",
        "\n",
        "Now, you will selectively restore weights from your checkpoint.\n",
        "- Your end goal is to create a custom model which reuses parts of, but not all of the layers of RetinaNet (currently stored in the variable `detection_model`.)\n",
        "  - The parts of RetinaNet that you want to reuse are:\n",
        "    - Feature extraction layers\n",
        "    - Bounding box regression prediction layer\n",
        "  - The part of RetinaNet that you will not want to reuse is the classification prediction layer (since you will define and train your own classification layer specific to zombies).\n",
        "  - For the parts of RetinaNet that you want to reuse, you will also restore the weights from the checkpoint that you selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISYvzjTcheJa"
      },
      "source": [
        "#### Inspect the detection_model\n",
        "First, take a look at the type of the detection_model and its Python class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M81_BgsuheJa"
      },
      "source": [
        "# Run this to check the type of detection_model\n",
        "detection_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxROGsrheJa"
      },
      "source": [
        "#### Find the source code for detection_model\n",
        "\n",
        "You'll see that the type of the model is `object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch`.\n",
        "Please practice some detective work and open up the source code for this class in GitHub repository. \n",
        "\n",
        "#### View the variables in detection_model\n",
        "Now, check the class variables that are in `detection_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6McB9xTheJa"
      },
      "source": [
        "vars(detection_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvLX9gheheJa"
      },
      "source": [
        "You'll see that detection_model contains several variables:\n",
        "\n",
        "Two of these will be relevant to you:\n",
        "```\n",
        "...\n",
        "_box_predictor': <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x7f5205eeb1d0>,\n",
        "...\n",
        "_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7f52040f1ef0>,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE907TPoheJa"
      },
      "source": [
        "#### Inspect `_feature_extractor`\n",
        "\n",
        "Take a look at the [ssd_meta_arch.py](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py) code.\n",
        "```\n",
        "# Line 302\n",
        "feature_extractor: a SSDFeatureExtractor object.\n",
        "```\n",
        "Also\n",
        "```\n",
        "# Line 380\n",
        "self._feature_extractor = feature_extractor\n",
        "```\n",
        "So `detection_model._feature_extractor` is a feature extractor, which you will want to reuse for your zombie detector model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzKgYA0oheJa"
      },
      "source": [
        "#### Inspect `_box_predictor`\n",
        "\n",
        "- View the [ssd_meta_arch.py](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/ssd_meta_arch.py) file (which is the source code for detection_model)\n",
        "- Notice that in the __init__ constructor for class SSDMetaArch(model.DetectionModel), \n",
        "```\n",
        "...\n",
        "box_predictor: a box_predictor.BoxPredictor object\n",
        "...\n",
        "self._box_predictor = box_predictor\n",
        "```\n",
        "#### Inspect _box_predictor\n",
        "Please take a look at the class type of `detection_model._box_predictor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToM0KgeMheJa"
      },
      "source": [
        "# view the type of _box_predictor\n",
        "detection_model._box_predictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAxQGzZ1heJa"
      },
      "source": [
        "You'll see that the class type of _box_predictor is\n",
        "```\n",
        "object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor\n",
        "```\n",
        "You can navigate through the GitHub repository to this path:\n",
        "- [objection_detection/predictors](https://github.com/tensorflow/models/tree/master/research/object_detection/predictors)\n",
        "- Notice that there is a file named convolutional_keras_box_predictor.py.  Please open that file.\n",
        "\n",
        "#### View variables in `_box_predictor`\n",
        "Also view the variables contained in _box_predictor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od3eQek-heJa"
      },
      "source": [
        "vars(detection_model._box_predictor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ79nKuoheJa"
      },
      "source": [
        "Among the variables listed, a few will be relevant to you:\n",
        "\n",
        "```\n",
        "...\n",
        "_base_tower_layers_for_heads\n",
        "...\n",
        "_box_prediction_head\n",
        "...\n",
        "_prediction_heads\n",
        "```\n",
        "\n",
        "In the source code for [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py) that you just opened, look at the source code to get a sense for what these three variables represent.\n",
        "\n",
        "#### Inspect `base_tower_layers_for_heads`\n",
        "If you look at the [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py) file, you'll notice this:\n",
        "```\n",
        "# line 302\n",
        "self._base_tower_layers_for_heads = {\n",
        "        BOX_ENCODINGS: [],\n",
        "        CLASS_PREDICTIONS_WITH_BACKGROUND: [],\n",
        "    }\n",
        "```\n",
        "- `base_tower_layers_for_heads` is a dictionary with two key-value pairs.\n",
        "  - `BOX_ENCODINGS`: points to a list of layers\n",
        "  - `CLASS_PREDICTIONS_WITH_BACKGROUND`: points to a list of layers\n",
        "  - If you scan the code, you'll see that for both of these, the lists are filled with all layers that appear BEFORE the prediction layer.\n",
        "```\n",
        "# Line 377\n",
        "# Stack the base_tower_layers in the order of conv_layer, batch_norm_layer\n",
        "    # and activation_layer\n",
        "    base_tower_layers = []\n",
        "    for i in range(self._num_layers_before_predictor):\n",
        "```\n",
        "\n",
        "So `detection_model.box_predictor._base_tower_layers_for_heads` contains:\n",
        "- The layers for the prediction before the final bounding box prediction\n",
        "- The layers for the prediction before the final class prediction.\n",
        "\n",
        "You will want to use these in your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4lxibHheJa"
      },
      "source": [
        "#### Inspect `_box_prediction_head`\n",
        "If you again look at [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py) file, you'll see this\n",
        "\n",
        "```\n",
        "# Line 248\n",
        "box_prediction_head: The head that predicts the boxes.\n",
        "```\n",
        "So `detection_model.box_predictor._box_prediction_head` points to the bounding box prediction layer, which you'll want to use for your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGEGC-d-heJa"
      },
      "source": [
        "#### Inspect `_prediction_heads`\n",
        "\n",
        "If you again look at [convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py) file, you'll see this\n",
        "```\n",
        "# Line 121\n",
        "self._prediction_heads = {\n",
        "        BOX_ENCODINGS: box_prediction_heads,\n",
        "        CLASS_PREDICTIONS_WITH_BACKGROUND: class_prediction_heads,\n",
        "    }\n",
        "```\n",
        "You'll also see this docstring\n",
        "```\n",
        "# Line 83\n",
        "class_prediction_heads: A list of heads that predict the classes.\n",
        "```\n",
        "\n",
        "So `detection_model.box_predictor._prediction_heads` is a dictionary that points to both prediction layers:\n",
        "- The layer that predicts the bounding boxes\n",
        "- The layer that predicts the class (category).\n",
        "\n",
        "#### Which layers will you reuse?\n",
        "Remember that you are reusing the model for its feature extraction and bounding box detection.\n",
        "- You will create your own classification layer and train it on zombie images.\n",
        "- So you won't need to reuse the class prediction layer of `detection_model`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGtYFYkQheJa"
      },
      "source": [
        "## Define checkpoints for desired layers\n",
        "You will now isolate the layers of `detection_model` that you wish to reuse so that you can restore the weights to just those layers.\n",
        "- First, define checkpoints for the box predictor\n",
        "- Next, define checkpoints for the model, which will point to this box predictor checkpoint as well as the feature extraction layers.\n",
        "\n",
        "Please use [tf.train.Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BK06DyFheJa"
      },
      "source": [
        "<a name='exercise-6-1'></a>\n",
        "### Define Checkpoints for the box predictor\n",
        "\n",
        "- Please define `box_predictor_checkpoint` to be checkpoint for these two layers of the `detection_model`'s box predictor:\n",
        "  - The base tower layer (the layers the precede both the class prediction and bounding box prediction layers).\n",
        "  - The box prediction head (the prediction layer for bounding boxes).\n",
        "- Note, you won't include the class prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KPWWYbyrkJY"
      },
      "source": [
        "tmp_box_predictor_checkpoint = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlefNC_XheJa"
      },
      "source": [
        "# Check the datatype of this checkpoint\n",
        "type(tmp_box_predictor_checkpoint)\n",
        "\n",
        "# Expected output:\n",
        "# tensorflow.python.training.tracking.util.Checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9X0BfufheJa"
      },
      "source": [
        "# Check the variables of this checkpoint\n",
        "vars(tmp_box_predictor_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORTI9PU6heJa"
      },
      "source": [
        "#### Expected output\n",
        "You should expect to see a list of variables that include the following:\n",
        "```\n",
        "'_base_tower_layers_for_heads': DictWrapper({'box_encodings': ListWrapper([]), 'class_predictions_with_background': ListWrapper([])}),\n",
        "'_box_prediction_head': <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x7fefac014710>,\n",
        " ... \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqMyPTU1rkJa"
      },
      "source": [
        "<a name='exercise-6-2'></a>\n",
        "### Define the temporary model checkpoint**\n",
        "\n",
        "Now define `tmp_model_checkpoint` so that it points to these two layers:\n",
        "- The feature extractor of the detection model.\n",
        "- The temporary box predictor checkpoint that you just defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqGfrawrkJa"
      },
      "source": [
        "tmp_model_checkpoint = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=tmp_box_predictor_checkpoint)        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXZBgRtheJa"
      },
      "source": [
        "# Check the datatype of this checkpoint\n",
        "type(tmp_model_checkpoint)\n",
        "\n",
        "# Expected output\n",
        "# tensorflow.python.training.tracking.util.Checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSxZqzU4heJa"
      },
      "source": [
        "# Check the vars of this checkpoint\n",
        "vars(tmp_model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJzbL4iheJa"
      },
      "source": [
        "#### Expected output\n",
        "Among the variables of this checkpoint, you should see:\n",
        "```\n",
        "'_box_predictor': <tensorflow.python.training.tracking.util.Checkpoint at 0x7fefac044a20>,\n",
        " '_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x7fefac0240b8>,\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_BtL4ZZKVvV"
      },
      "source": [
        "<a name='exercise-6-3'></a>\n",
        "### Restore the checkpoint\n",
        "\n",
        "You can now restore the checkpoint.\n",
        "\n",
        "First, find and set the `checkpoint_path`\n",
        "\n",
        "- checkpoint_path: \n",
        "- Please set checkpoint_path to the path to the full path `models/.../ckpt-0` \n",
        "\n",
        "Next, define one last checkpoint using `tf.train.Checkpoint()`.\n",
        "- For the single keyword argument, \n",
        "  - Set the key as `model=` \n",
        "  - Set the value to your temporary model checkpoint that you just defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkvDUUarkJg"
      },
      "source": [
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# Define a checkpoint that sets `model= None\n",
        "checkpoint = tf.compat.v2.train.Checkpoint(model=tmp_model_checkpoint)\n",
        "\n",
        "# Restore the checkpoint to the checkpoint path\n",
        "checkpoint.restore(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k6oFoxTrkJi"
      },
      "source": [
        "<a name='exercise-7'></a>\n",
        "### Run a dummy image to generate the model variables\n",
        "\n",
        "Run a dummy image through the model so that variables are created.   \n",
        "  \n",
        "**Note**: Please use the recommended variable names, which include the prefix `tmp_`, since these variables won't be used later, but you'll define similarly-named variables later for predicting on actual zombie images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MANTSP9LrkJi"
      },
      "source": [
        "# use the detection model's `preprocess()` method and pass a dummy image\n",
        "tmp_image, tmp_shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "\n",
        "# run a prediction with the preprocessed image and shapes\n",
        "tmp_prediction_dict = detection_model.predict(tmp_image, tmp_shapes)\n",
        "\n",
        "# postprocess the predictions into final detections\n",
        "tmp_detections = detection_model.postprocess(tmp_prediction_dict, tmp_shapes)\n",
        "\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0KvPP-krkJn"
      },
      "source": [
        "# Test Code:\n",
        "assert len(detection_model.trainable_variables) > 0, \"Please pass in a dummy image to create the trainable variables.\"\n",
        "\n",
        "print(detection_model.weights[0].shape)\n",
        "print(detection_model.weights[231].shape)\n",
        "print(detection_model.weights[462].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCJXBqROrkJp"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```txt\n",
        "(3, 3, 256, 24)\n",
        "(512,)\n",
        "(256,)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCkWmdoZZ0zJ"
      },
      "source": [
        "## Eager mode custom training loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlV4PoPrkJq"
      },
      "source": [
        "<a name='exercise-8'></a>\n",
        "### Set training hyperparameters\n",
        "\n",
        "Set an appropriate learning rate and optimizer for the training. \n",
        "\n",
        "- batch_size: you can use 4\n",
        "  - You can increase the batch size up to 5, since you have just 5 images for training.\n",
        "- num_batches: You can use 100\n",
        "  - You can increase the number of batches but the training will take longer to complete. \n",
        "- learning_rate: You can use 0.01\n",
        "- optimizer: you can use [tf.keras.optimizers.SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)\n",
        "  - Set the learning rate\n",
        "  - Set the momentum to 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyHoF4mUrv5-"
      },
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "# set the batch_size\n",
        "batch_size = 4\n",
        "\n",
        "# set the number of batches\n",
        "num_batches = 100\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.008\n",
        "\n",
        "# set the optimizer and pass in the learning_rate\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MaJmHmrkJs"
      },
      "source": [
        "## Choose the layers to fine-tune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91r2dk-7heJb"
      },
      "source": [
        "# Inspect the layers of detection_model\n",
        "for i,v in enumerate(detection_model.trainable_variables):\n",
        "    print(f\"i: {i} \\t name: {v.name} \\t shape:{v.shape} \\t dtype={v.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wtU67JZheJb"
      },
      "source": [
        "Notice that there are some layers whose names are prefixed with the following:\n",
        "```\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/BoxPredictionTower\n",
        "...\n",
        "WeightSharedConvolutionalBoxPredictor/ClassPredictionTower\n",
        "...\n",
        "```\n",
        "\n",
        "Among these, which do you think are the prediction layers at the \"end\" of the model?\n",
        "- Recall that when inspecting the source code to restore the checkpoints ([convolutional_keras_box_predictor.py](https://github.com/tensorflow/models/blob/master/research/object_detection/predictors/convolutional_keras_box_predictor.py)) you noticed that:\n",
        "  - `_base_tower_layers_for_heads`: refers to the layers that are placed right before the prediction layer\n",
        "  - `_box_prediction_head` refers to the prediction layer for the bounding boxes\n",
        "  - `_prediction_heads`: refers to the set of prediction layers (both for classification and for bounding boxes)\n",
        "\n",
        "\n",
        "So you can see that in the source code for this model, \"tower\" refers to layers that are before the prediction layer, and \"head\" refers to the prediction layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6S9QjSWheJb"
      },
      "source": [
        "<a name='exercise-9'></a>\n",
        "\n",
        "### Select the prediction layer variables\n",
        "\n",
        "Based on inspecting the `detection_model.trainable_variables`, please select the prediction layer variables that you will fine tune:\n",
        "- The bounding box head variables (which predict bounding box coordinates)\n",
        "- The class head variables (which predict the class/category)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDdvGRCYrkJt"
      },
      "source": [
        "# define a list that contains the layers that you wish to fine tune\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "\n",
        "to_fine_tune = []\n",
        "\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwzzGkzyrkJv"
      },
      "source": [
        "# Test Code:\n",
        "\n",
        "print(to_fine_tune[0].name)\n",
        "print(to_fine_tune[2].name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-VK-GB2rkJy"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```txt\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead/BoxPredictor/kernel:0\n",
        "WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead/ClassPredictor/kernel:0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvhZZu2rkJy"
      },
      "source": [
        "## Train your model\n",
        "\n",
        "You'll define a function that handles training for one batch, which you'll later use in your training loop.\n",
        "\n",
        "First, walk through these code cells to learn how you'll perform training using this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qJcTJTvheJb"
      },
      "source": [
        "# Get a batch of your training images\n",
        "g_images_list = train_image_tensors[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw44cJLcheJb"
      },
      "source": [
        "The `detection_model` is of class [SSDMetaArch](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L655), and its source code shows that is has this function [preprocess](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L459).\n",
        "- This preprocesses the images so that they can be passed into the model (for training or prediction):\n",
        "```\n",
        "  def preprocess(self, inputs):\n",
        "    \"\"\"Feature-extractor specific preprocessing.\n",
        "    ...\n",
        "    Args:\n",
        "      inputs: a [batch, height_in, width_in, channels] float tensor representing\n",
        "        a batch of images with values between 0 and 255.0.\n",
        "    Returns:\n",
        "      preprocessed_inputs: a [batch, height_out, width_out, channels] float\n",
        "        tensor representing a batch of images.\n",
        "        \n",
        "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
        "        of the form [height, width, channels] indicating the shapes\n",
        "        of true images in the resized images, as resized images can be padded\n",
        "        with zeros.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XifEma9DheJb"
      },
      "source": [
        "# Use .preprocess to preprocess an image\n",
        "g_preprocessed_image = detection_model.preprocess(g_images_list[0])\n",
        "print(f\"g_preprocessed_image type: {type(g_preprocessed_image)}\")\n",
        "print(f\"g_preprocessed_image length: {len(g_preprocessed_image)}\")\n",
        "print(f\"index 0 has the preprocessed image of shape {g_preprocessed_image[0].shape}\")\n",
        "print(f\"index 1 has information about the image's true shape excluding padding: {g_preprocessed_image[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kQq7BrMheJb"
      },
      "source": [
        "You can pre-process each image and save their outputs into two separate lists\n",
        "- One list of the preprocessed images\n",
        "- One list of the true shape for each preprocessed image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXKZR5rOheJb"
      },
      "source": [
        "preprocessed_image_list = []\n",
        "true_shape_list = []\n",
        "\n",
        "for img in g_images_list:\n",
        "    processed_img, true_shape = detection_model.preprocess(img)\n",
        "    preprocessed_image_list.append(processed_img)\n",
        "    true_shape_list.append(true_shape)\n",
        "\n",
        "print(f\"preprocessed_image_list is of type {type(preprocessed_image_list)}\")\n",
        "print(f\"preprocessed_image_list has length {len(preprocessed_image_list)}\")\n",
        "print()\n",
        "print(f\"true_shape_list is of type {type(true_shape_list)}\")\n",
        "print(f\"true_shape_list has length {len(true_shape_list)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpyb8CyKheJb"
      },
      "source": [
        "## Make a prediction\n",
        "The `detection_model` also has a `.predict` function.  According to the source code for [predict](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L525)\n",
        "\n",
        "```\n",
        "  def predict(self, preprocessed_inputs, true_image_shapes):\n",
        "    \"\"\"Predicts unpostprocessed tensors from input tensor.\n",
        "    This function takes an input batch of images and runs it through the forward\n",
        "    pass of the network to yield unpostprocessesed predictions.\n",
        "...\n",
        "    Args:\n",
        "      preprocessed_inputs: a [batch, height, width, channels] image tensor.\n",
        "      \n",
        "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
        "        of the form [height, width, channels] indicating the shapes\n",
        "        of true images in the resized images, as resized images can be padded\n",
        "        with zeros.\n",
        "        \n",
        "    Returns:\n",
        "      prediction_dict: a dictionary holding \"raw\" prediction tensors:\n",
        "        1) preprocessed_inputs: the [batch, height, width, channels] image\n",
        "          tensor.\n",
        "        2) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,\n",
        "          box_code_dimension] containing predicted boxes.\n",
        "        3) class_predictions_with_background: 3-D float tensor of shape\n",
        "          [batch_size, num_anchors, num_classes+1] containing class predictions\n",
        "          (logits) for each of the anchors.  Note that this tensor *includes*\n",
        "          background class predictions (at class index 0).\n",
        "        4) feature_maps: a list of tensors where the ith tensor has shape\n",
        "          [batch, height_i, width_i, depth_i].\n",
        "        5) anchors: 2-D float tensor of shape [num_anchors, 4] containing\n",
        "          the generated anchors in normalized coordinates.\n",
        "        6) final_anchors: 3-D float tensor of shape [batch_size, num_anchors, 4]\n",
        "          containing the generated anchors in normalized coordinates.\n",
        "        If self._return_raw_detections_during_predict is True, the dictionary\n",
        "        will also contain:\n",
        "        7) raw_detection_boxes: a 4-D float32 tensor with shape\n",
        "          [batch_size, self.max_num_proposals, 4] in normalized coordinates.\n",
        "        8) raw_detection_feature_map_indices: a 3-D int32 tensor with shape\n",
        "          [batch_size, self.max_num_proposals].\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "Notice that `.predict` takes its inputs as tensors.  If you tried to pass in the preprocessed images and true shapes, you'll get an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "5oAoT6q0heJb"
      },
      "source": [
        "# Try to call `predict` and pass in lists; look at the error message\n",
        "try:\n",
        "    detection_model.predict(preprocessed_image_list, true_shape_list)\n",
        "except AttributeError as e:\n",
        "    print(\"Error message:\", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Bs4lfPheJb"
      },
      "source": [
        "But don't worry! You can check how to properly use `predict`:\n",
        "- Notice that the source code documentation says that `preprocessed_inputs` and `true_image_shapes` are expected to be tensors and not lists of tensors.\n",
        "- One way to turn a list of tensors into a tensor is to use [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat)\n",
        "\n",
        "```\n",
        "tf.concat(\n",
        "    values, axis, name='concat'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He2IbVteheJb"
      },
      "source": [
        "# Turn a list of tensors into a tensor\n",
        "preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "print(f\"preprocessed_image_tensor shape: {preprocessed_image_tensor.shape}\")\n",
        "print(f\"true_shape_tensor shape: {true_shape_tensor.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCkoOWXdheJb"
      },
      "source": [
        "Now you can make predictions for the images.\n",
        "According to the source code, `predict` returns a dictionary containing the prediction information, including:\n",
        "- The bounding box predictions\n",
        "- The class predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSiOe-FFheJb"
      },
      "source": [
        "# Make predictions on the images\n",
        "prediction_dict = detection_model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "print(\"keys in prediction_dict:\")\n",
        "for key in prediction_dict.keys():\n",
        "    print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amiRcKs8heJb"
      },
      "source": [
        "#### Calculate loss\n",
        "\n",
        "Now that your model has made its prediction, you want to compare it to the ground truth in order to calculate a loss.\n",
        "- The `detection_model` has a [loss](https://github.com/tensorflow/models/blob/dc4d11216b738920ddb136729e3ae71bddb75c7e/research/object_detection/meta_architectures/ssd_meta_arch.py#L807) function.\n",
        "\n",
        "```Python\n",
        "  def loss(self, prediction_dict, true_image_shapes, scope=None):\n",
        "    \"\"\"Compute scalar loss tensors with respect to provided groundtruth.\n",
        "    Calling this function requires that groundtruth tensors have been\n",
        "    provided via the provide_groundtruth function.\n",
        "    Args:\n",
        "      prediction_dict: a dictionary holding prediction tensors with\n",
        "        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,\n",
        "          box_code_dimension] containing predicted boxes.\n",
        "        2) class_predictions_with_background: 3-D float tensor of shape\n",
        "          [batch_size, num_anchors, num_classes+1] containing class predictions\n",
        "          (logits) for each of the anchors. Note that this tensor *includes*\n",
        "          background class predictions.\n",
        "      true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
        "        of the form [height, width, channels] indicating the shapes\n",
        "        of true images in the resized images, as resized images can be padded\n",
        "        with zeros.\n",
        "      scope: Optional scope name.\n",
        "    Returns:\n",
        "      a dictionary mapping loss keys (`localization_loss` and\n",
        "        `classification_loss`) to scalar tensors representing corresponding loss\n",
        "        values.\n",
        "    \"\"\"\n",
        "```\n",
        "It takes in:\n",
        "- The prediction dictionary that comes from your call to `.predict()`.\n",
        "- the true images shape that comes from your call to `.preprocess()` followed by the conversion from a list to a tensor.\n",
        "\n",
        "Try calling `.loss`.  You'll see an error message that you'll addres in order to run the `.loss` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPeDlc6VheJb"
      },
      "source": [
        "try:\n",
        "    losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMeqXdjSheJb"
      },
      "source": [
        "This is giving an error about groundtruth_classes_list: \n",
        "```\n",
        "The graph tensor has name: groundtruth_classes_list:0\n",
        "```\n",
        "\n",
        "Notice in the docstring for `loss` (shown above), it says:\n",
        "```\n",
        "Calling this function requires that groundtruth tensors have been\n",
        "    provided via the provide_groundtruth function.\n",
        "```\n",
        "\n",
        "So you'll first want to set the ground truth (true labels and true bounding boxes) before you calculate the loss.\n",
        "- This makes sense, since the loss is comparing the prediction to the ground truth, and so the loss function needs to know the ground truth.\n",
        "#### Provide the ground truth\n",
        "The source code for providing the ground truth is located in the parent class of `SSDMetaArch`, `model.DetectionModel`.\n",
        "- Here is the link to the code for [provide_ground_truth](https://github.com/tensorflow/models/blob/fd6b24c19c68af026bb0a9efc9f7b1719c231d3d/research/object_detection/core/model.py#L297)\n",
        "\n",
        "```Python\n",
        "def provide_groundtruth(\n",
        "      self,\n",
        "      groundtruth_boxes_list,\n",
        "      groundtruth_classes_list,\n",
        "... # more parameters not show here\n",
        "\"\"\"\n",
        "    Args:\n",
        "      groundtruth_boxes_list: a list of 2-D tf.float32 tensors of shape\n",
        "        [num_boxes, 4] containing coordinates of the groundtruth boxes.\n",
        "          Groundtruth boxes are provided in [y_min, x_min, y_max, x_max]\n",
        "          format and assumed to be normalized and clipped\n",
        "          relative to the image window with y_min <= y_max and x_min <= x_max.\n",
        "      groundtruth_classes_list: a list of 2-D tf.float32 one-hot (or k-hot)\n",
        "        tensors of shape [num_boxes, num_classes] containing the class targets\n",
        "        with the 0th index assumed to map to the first non-background class.\n",
        "\"\"\"\n",
        "```\n",
        "You'll set two parameters in `provide_ground_truth`:\n",
        "- The true bounding boxes\n",
        "- The true classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XWjDnRxheJb"
      },
      "source": [
        "# Get the ground truth bounding boxes\n",
        "gt_boxes_list = gt_box_tensors[0:2]\n",
        "\n",
        "# Get the ground truth class labels\n",
        "gt_classes_list = gt_classes_one_hot_tensors[0:2]\n",
        "\n",
        "# Provide the ground truth to the model\n",
        "detection_model.provide_groundtruth(\n",
        "            groundtruth_boxes_list=gt_boxes_list,\n",
        "            groundtruth_classes_list=gt_classes_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gjgxv9ZheJb"
      },
      "source": [
        "Now you can calculate the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-bztzeNheJb"
      },
      "source": [
        "# Calculate the loss after you've provided the ground truth \n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "\n",
        "# View the loss dictionary\n",
        "losses_dict = detection_model.loss(prediction_dict, true_shape_tensor)\n",
        "print(f\"loss dictionary keys: {losses_dict.keys()}\")\n",
        "print(f\"localization loss {losses_dict['Loss/localization_loss']:.8f}\")\n",
        "print(f\"classification loss {losses_dict['Loss/classification_loss']:.8f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHC1vJsLheJb"
      },
      "source": [
        "You can now calculate the gradient and optimize the variables that you selected to fine tune.\n",
        "- Use tf.GradientTape\n",
        "\n",
        "```Python\n",
        "with tf.GradientTape() as tape:\n",
        "    # Make the prediction \n",
        "    \n",
        "    # calculate the loss\n",
        "        \n",
        "    # calculate the gradient of each model variable with respect to each loss\n",
        "    gradients = tape.gradient([some loss], variables to fine tune)\n",
        "    \n",
        "    # apply the gradients to update these model variables\n",
        "    optimizer.apply_gradients(zip(gradients, variables to fine tune))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdY8qHSLheJb"
      },
      "source": [
        "# Let's just reset the model so that you can practice setting it up yourself!\n",
        "detection_model.provide_groundtruth(groundtruth_boxes_list=[], groundtruth_classes_list=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIUKsYPLheJb"
      },
      "source": [
        "<a name='exercise-10'></a>\n",
        "\n",
        "### Define the training step\n",
        "Please complete the function below to set up one training step.\n",
        "- Preprocess the images\n",
        "- Make a prediction\n",
        "- Calculate the loss (and make sure the loss function has the ground truth to compare with the prediction)\n",
        "- Calculate the total loss:\n",
        "  - `total_loss` = `localization_loss + classification_loss`\n",
        "  - Note: this is different than the example code that you saw above\n",
        "- Calculate gradients with respect to the variables you selected to train.\n",
        "- Optimize the model's variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "nttIf_ZgheJb"
      },
      "source": [
        "# decorate with @tf.function for faster training (remember, graph mode!)\n",
        "@tf.function\n",
        "def train_step_fn(image_list,\n",
        "                groundtruth_boxes_list,\n",
        "                groundtruth_classes_list,\n",
        "                model,\n",
        "                optimizer,\n",
        "                vars_to_fine_tune):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_list: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 640x640.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "    ### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "\n",
        "        # Preprocess the images\n",
        "        preprocessed_image_list = []\n",
        "        true_shape_list = []\n",
        "\n",
        "        for img in image_list:\n",
        "          processed_img, true_shape = detection_model.preprocess(img)\n",
        "          preprocessed_image_list.append(processed_img)\n",
        "          true_shape_list.append(true_shape)\n",
        "\n",
        "        preprocessed_image_tensor = tf.concat(preprocessed_image_list, axis=0)\n",
        "        true_shape_tensor = tf.concat(true_shape_list, axis=0)\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction_dict = model.predict(preprocessed_image_tensor, true_shape_tensor)\n",
        "\n",
        "        # Calculate the total loss (sum of both losses)\n",
        "        \n",
        "        # Get the ground truth bounding boxes\n",
        "        #gt_boxes_list = gt_box_tensors[0:5]\n",
        "\n",
        "        # Get the ground truth class labels\n",
        "        #gt_classes_list = gt_classes_one_hot_tensors[0:5]\n",
        "\n",
        "        # Provide the ground truth to the model\n",
        "        model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "            \n",
        "        \n",
        "\n",
        "        losses_dict = model.loss(prediction_dict, true_shape_tensor)\n",
        "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "\n",
        "        # Calculate the gradients\n",
        "        gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "\n",
        "        # Optimize the model's selected variables\n",
        "        optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n7imaJRrkJ1"
      },
      "source": [
        "## Run the training loop\n",
        "\n",
        "Run the training loop using the training step function that you just defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgpDQ3JbrkJ3"
      },
      "source": [
        "print('Start fine-tuning!', flush=True)\n",
        "\n",
        "for idx in range(num_batches):\n",
        "    # Grab keys for a random subset of examples\n",
        "    all_keys = list(range(len(train_images_np)))\n",
        "    random.shuffle(all_keys)\n",
        "    example_keys = all_keys[:batch_size]\n",
        "\n",
        "    # Get the ground truth\n",
        "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
        "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
        "    \n",
        "    # get the images\n",
        "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
        "\n",
        "    # Training step (forward pass + backwards pass)\n",
        "    total_loss = train_step_fn(image_tensors, \n",
        "                               gt_boxes_list, \n",
        "                               gt_classes_list,\n",
        "                               detection_model,\n",
        "                               optimizer,\n",
        "                               to_fine_tune\n",
        "                              )\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "        + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx8KefSZrkJ5"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "Total loss should be decreasing and should be less than 1 after fine tuning. For example:\n",
        "\n",
        "```txt\n",
        "Start fine-tuning!\n",
        "batch 0 of 100, loss=1.2559178\n",
        "batch 10 of 100, loss=16.067217\n",
        "batch 20 of 100, loss=8.094654\n",
        "batch 30 of 100, loss=0.34514275\n",
        "batch 40 of 100, loss=0.033170983\n",
        "batch 50 of 100, loss=0.0024622646\n",
        "batch 60 of 100, loss=0.00074224477\n",
        "batch 70 of 100, loss=0.0006149876\n",
        "batch 80 of 100, loss=0.00046916265\n",
        "batch 90 of 100, loss=0.0004159231\n",
        "Done fine-tuning!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHlXL1x_Z3tc"
      },
      "source": [
        "## Load test images and run inference with new model!\n",
        "\n",
        "You can now test your model on a new set of images. The cell below downloads 237 images of a walking zombie and stores them in a `results/` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL697_MnHcJF"
      },
      "source": [
        "# uncomment if you want to delete existing files\n",
        "!rm zombie-walk-frames.zip\n",
        "!rm -rf ./zombie-walk\n",
        "!rm -rf ./results\n",
        "\n",
        "# download test images\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/zombie-walk-frames.zip \\\n",
        "    -O zombie-walk-frames.zip\n",
        "\n",
        "# unzip test images\n",
        "local_zip = './zombie-walk-frames.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./results')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LJJ8wWhzlZ"
      },
      "source": [
        "You will load these images into numpy arrays to prepare it for inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcE6OwrHQJya"
      },
      "source": [
        "test_image_dir = './results/'\n",
        "test_images_np = []\n",
        "\n",
        "# load images into a numpy array. this will take a few minutes to complete.\n",
        "for i in range(0, 237):\n",
        "    image_path = os.path.join(test_image_dir, 'zombie-walk' + \"{0:04}\".format(i) + '.jpg')\n",
        "    print(image_path)\n",
        "    test_images_np.append(np.expand_dims(\n",
        "      load_image_into_numpy_array(image_path), axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GIbqkzrkJ_"
      },
      "source": [
        "<a name='exercise-11'></a>\n",
        "\n",
        "### Preprocess, predict, and post process an image\n",
        "\n",
        "Define a function that returns the detection boxes, classes, and scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aXt-bodrkKA"
      },
      "source": [
        "# Again, uncomment this decorator if you want to run inference eagerly\n",
        "@tf.function\n",
        "def detect(input_tensor):\n",
        "    \"\"\"Run detection on an input image.\n",
        "\n",
        "    Args:\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "\n",
        "    Returns:\n",
        "    A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
        "      and `detection_scores`).\n",
        "    \"\"\"\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "    # use the detection model's postprocess() method to get the the final detections\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzvMrohqiyZD"
      },
      "source": [
        "You can now loop through the test images and get the detection scores and bounding boxes to overlay in the original image. We will save each result in a `results` dictionary and the autograder will use this to evaluate your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhXwX7O8rkKC"
      },
      "source": [
        "# Note that the first frame will trigger tracing of the tf.function, which will\n",
        "# take some time, after which inference should be fast.\n",
        "\n",
        "label_id_offset = 1\n",
        "results = {'boxes': [], 'scores': []}\n",
        "\n",
        "for i in range(len(test_images_np)):\n",
        "    input_tensor = tf.convert_to_tensor(test_images_np[i], dtype=tf.float32)\n",
        "    detections = detect(input_tensor)\n",
        "    plot_detections(\n",
        "      test_images_np[i][0],\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      detections['detection_classes'][0].numpy().astype(np.uint32)\n",
        "      + label_id_offset,\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index, figsize=(15, 20), image_name=\"./results/gif_frame_\" + ('%03d' % i) + \".jpg\")\n",
        "    results['boxes'].append(detections['detection_boxes'][0][0].numpy())\n",
        "    results['scores'].append(detections['detection_scores'][0][0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNV5uwN7rkKE"
      },
      "source": [
        "# TEST CODE\n",
        "\n",
        "print(len(results['boxes']))\n",
        "print(results['boxes'][0].shape)\n",
        "print()\n",
        "\n",
        "# compare with expected bounding boxes\n",
        "print(np.allclose(results['boxes'][0], [0.28838485, 0.06830047, 0.7213766 , 0.19833465], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][5], [0.29168868, 0.07529271, 0.72504973, 0.20099735], rtol=0.18))\n",
        "print(np.allclose(results['boxes'][10], [0.29548776, 0.07994056, 0.7238164 , 0.20778716], rtol=0.18))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRbmLv_UrkKG"
      },
      "source": [
        "**Expected Output:** Ideally the three boolean values at the bottom should be `True`. But if you only get two, you can still try submitting. This compares your resulting bounding boxes for each zombie image to some preloaded coordinates (i.e. the hardcoded values in the test cell above). Depending on how you annotated the training images,it's possible that some of your results differ for these three frames but still get good results overall when all images are examined by the grader. If two or all are False, please try annotating the images again with a tighter bounding box or use the [predefined `gt_boxes` list](#gt-boxes).\n",
        "\n",
        "```txt\n",
        "237\n",
        "(4,)\n",
        "\n",
        "True\n",
        "True\n",
        "True\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Rh0oy7l7Aj"
      },
      "source": [
        "You can also check if the model detects a zombie class in the images by examining the `scores` key of the `results` dictionary. You should get higher than 88.0 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEYAkg3l4lh"
      },
      "source": [
        "x = np.array(results['scores'])\n",
        "\n",
        "# percent of frames where a zombie is detected\n",
        "zombie_detected = (np.where(x > 0.9, 1, 0).sum())/237*100\n",
        "print(zombie_detected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ3JQiRWj3FH"
      },
      "source": [
        "You can also display some still frames and inspect visually. If you don't see a bounding box around the zombie, please consider re-annotating the ground truth or use the predefined `gt_boxes` [here](#gt-boxes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww0hLDI4fB1f"
      },
      "source": [
        "print('Frame 0')\n",
        "display(IPyImage('./results/gif_frame_020.jpg'))\n",
        "print()\n",
        "print('Frame 5')\n",
        "display(IPyImage('./results/gif_frame_055.jpg'))\n",
        "print()\n",
        "print('Frame 10')\n",
        "display(IPyImage('./results/gif_frame_080.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrjHwbkVXqLG"
      },
      "source": [
        "## Create a zip of the zombie-walk images. \n",
        "You can download this if you like to create your own animations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeKSNOG7BVd4"
      },
      "source": [
        "zipf = zipfile.ZipFile('./zombie.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "\n",
        "for filename in filenames:\n",
        "    zipf.write(filename)\n",
        "\n",
        "zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDH45KYX1Iv"
      },
      "source": [
        "## Create Zombie animation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW1FrT2iNnpy"
      },
      "source": [
        "imageio.plugins.freeimage.download()\n",
        "\n",
        "!rm -rf ./results/zombie-anim.gif\n",
        "\n",
        "anim_file = './zombie-anim.gif'\n",
        "\n",
        "filenames = glob.glob('./results/gif_frame_*.jpg')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "images = []\n",
        "\n",
        "for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    images.append(image)\n",
        "\n",
        "imageio.mimsave(anim_file, images, 'GIF-FI', fps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JqLoPaigdl4"
      },
      "source": [
        "Unfortunately, using `IPyImage` in the notebook (as you've done in the rubber ducky detection tutorial) for the large `gif` generated will disconnect the runtime. To view the animation, you can instead use the `Files` pane on the left and double-click on `zombie-anim.gif`. That will open a preview page on the right. It will take 2 to 3 minutes to load and see the walking zombie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdRXjK-xhTDA"
      },
      "source": [
        "## Save results file for grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxx-hJlkhvNs"
      },
      "source": [
        "Run the cell below to save your results. Download the `results.data` file and upload it to the grader in the classroom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0PbSjPXrkKN"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# remove file if it exists\n",
        "!rm results.data\n",
        "\n",
        "# write results to binary file. upload for grading.\n",
        "with open('results.data', 'wb') as filehandle:\n",
        "    pickle.dump(results['boxes'], filehandle)\n",
        "\n",
        "print('Done saving! Please download `results.data` from the Files tab\\n' \\\n",
        "      'on the left and submit for grading.\\nYou can also use the next cell as a shortcut for downloading.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdow2tKAACF"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('results.data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJHinV3nS1O"
      },
      "source": [
        "**Congratulations on completing this assignment! Please go back to the Coursera classroom and upload `results.data` to the Graded Lab item for Week 2.**"
      ]
    }
  ]
}